\chapter{Analysis and Discussion}

\label{sec:analysis_discussion}

Building on the experimental results in Chapters~2 and 3, this chapter compares the three model families (BiLSTM-CRF, RoBERTa, DistilBERT-CRF), analyzes the role of structured CRF decoding, and discusses the main error patterns, stability properties, and remaining limitations observed in our experiments.

\section{Overall Performance and Architectural Trade-offs}

The results exhibit a clear performance hierarchy across architectures, while also revealing a nuanced trade-off between accuracy and efficiency. Table~\ref{tab:overall_results} summarizes the main quantitative outcomes for each model family.

\begin{table}[h]
\centering
\begin{tabular}{lccc}
\hline
Model & CV F1 (mean) & Test F1 & Notes \\
\hline
RoBERTa & -- & $91.8$--$92.1\%$ & Large pretrained transformer \\
DistilBERT-CRF & $92.84\% \pm 1.02\%$ & $\approx 89.6\%$ & Compact encoder + CRF \\
BiLSTM-CRF & up to $0.8317$ & -- & Non-pretrained baseline \\
\hline
\end{tabular}
\caption{Summary of model performance on the CoNLL-2003 English NER task. Cross-validation (CV) scores are averaged over five folds where applicable.}
\label{tab:overall_results}
\end{table}

Several observations follow from these results:

\begin{enumerate}
    \item \textbf{RoBERTa (Test F1 $\approx 91.8$--$92.1\%$)} \\
    The fine-tuned RoBERTa model achieves the highest test F1 score among all systems. Its advantage mainly comes from deeply contextualized representations learned from large-scale pre-training (about 160~GB of text), which allow it to capture long-range dependencies and perform robust word sense disambiguation. In our setting, RoBERTa serves as a strong reference point for what can be achieved with a full-sized transformer encoder.

    \item \textbf{DistilBERT-CRF (CV F1 = $92.84\% \pm 1.02\%$, Test F1 $\approx 89.6\%$)} \\
    DistilBERT-CRF attains a strong cross-validation performance that is slightly higher than RoBERTa's \emph{test} F1, but drops to about $89.6\%$ on the held-out test set. This suggests that the model fits the training folds well but is somewhat more sensitive to distribution shifts between the cross-validation folds and the CoNLL-2003 test split. While this does not imply that DistilBERT-CRF surpasses RoBERTa in an apples-to-apples comparison, it indicates that a compact encoder paired with a CRF decoder can reach competitive performance in terms of dev-time metrics. Given that DistilBERT has only about $60\%$ of BERT's parameters, this combination provides, in principle, an attractive balance between accuracy and computational cost.

    \item \textbf{BiLSTM-CRF (CV F1 up to $0.8317$)} \\
    The best BiLSTM-CRF configuration reaches an F1 of approximately $0.83$, which is clearly below both transformer-based models but still constitutes a reasonably strong non-pretrained baseline. This confirms that static embeddings plus recurrent encoding are limited in capturing rich semantic phenomena compared to large pretrained transformers, yet still useful as a sanity-check baseline and as a platform to study the impact of structured decoding.
\end{enumerate}

Overall, the experiments confirm that pretrained transformer encoders are indispensable for high-accuracy NER on CoNLL-2003, while CRF-based structured decoding remains highly beneficial, especially when model capacity is constrained.

\section{Necessity of CRF for BiLSTM-based Models}

The grid search in Section~3.1.4 reveals an extreme performance gap between BiLSTM-CRF and vanilla BiLSTM across all hyperparameter settings. While BiLSTM-CRF reaches F1 scores in the $0.75$--$0.83$ range, the corresponding vanilla BiLSTM models achieve only about \textbf{6--7\% F1} ($0.061$--$0.075$), effectively rendering them unusable in practice. This highlights two critical functions of the CRF layer:

\begin{enumerate}
    \item \textbf{Enforcing Label Sequence Validity} \\
    The dataset is originally annotated with BIO tags, which impose strict constraints such as \texttt{I-PER} being allowed only after \texttt{B-PER} or another \texttt{I-PER}. A token-wise softmax classifier treats labels independently and thus frequently produces illegal transitions (e.g., \texttt{O} $\rightarrow$ \texttt{I-LOC}), which are heavily penalized by sequence-level F1. In contrast, the linear-chain CRF learns a transition matrix that assigns low scores to invalid transitions and high scores to plausible ones, enforcing global consistency during Viterbi decoding. This substantially improves the BiLSTM-based system from near-random entity-level F1 to a usable NER model. Although the DistilBERT-CRF experiments adopt a BIOES variant, the underlying argument about CRF-enforced sequence validity still applies.

    \item \textbf{Mitigating the ``All-O'' Local Optimum under Label Imbalance} \\
    Table~1.2 shows that the majority of tokens are labeled as \texttt{O}, with entity tags being highly under-represented. Under purely token-wise training, the BiLSTM classifier is strongly biased towards predicting \texttt{O} everywhere, since such behavior already yields high token-level accuracy but extremely low entity-level F1. By modeling the joint probability of the entire label sequence, the CRF discourages trivial all-\texttt{O} sequences when the input contains strong local evidence for entities, thereby reducing the impact of label imbalance on the effective decision boundary.
\end{enumerate}

In summary, the BiLSTM encoder alone is insufficient for NER on this dataset; structured CRF decoding is essential to enforce legal BIO/BIOES patterns and to escape degenerate local optima induced by class imbalance.

\section{Error Patterns and Semantic Limitations}

Even though RoBERTa delivers the best overall test performance, the qualitative analysis in Section~3.2.4 and Figure~3.6 exposes systematic weaknesses that are also visible in manual inspection of misclassified cases. In this section we summarize the main error patterns and relate them to the underlying semantic limitations of the models.

\subsection{Over-generalization of Entity-like Surface Forms}

In one error example, the phrase ``WESTERN DIVISION'' is incorrectly tagged as an entity. This suggests that the model has over-generalized from training patterns where capitalized multi-word expressions often correspond to locations or organizations. When encountering visually similar phrases that are actually non-entities, RoBERTa tends to produce spurious entities, reflecting an over-reliance on surface cues such as capitalization and word shape. Similar behavior is occasionally observed for sports teams, competitions, and organizational units whose surface forms resemble proper names.

\subsection{Semantic Ambiguity and Type Confusion}

Another example shows ``GOLF'' being misclassified as a location. Such errors typically occur when the surrounding context is not sufficiently informative to disambiguate between activity, event, and place interpretations. Manual inspection suggests that type confusion (e.g., ORG vs.\ MISC vs.\ LOC) dominates the remaining error distribution, while pure boundary errors are relatively rare. This indicates that the main difficulty lies in assigning the correct entity type rather than detecting span boundaries.

These observations suggest that while contextualized transformers handle boundaries well, they still struggle with fine-grained semantic categorization, especially for ambiguous phrases and polysemous tokens. Potential remedies include:
\begin{itemize}
    \item incorporating external gazetteers or knowledge bases to ground ambiguous mentions,
    \item more aggressive hard-negative mining of non-entity spans that resemble entities, and
    \item contrastive or adversarial training objectives that explicitly separate entity types in representation space.
\end{itemize}

\subsection{Stability and Convergence of DistilBERT-CRF}

The DistilBERT-CRF model exhibits notably stable training dynamics:
\begin{itemize}
    \item The 5-fold cross-validation results (Mean F1 = $92.84\% \pm 1.02\%$) show low variance across folds, indicating robust generalization within the training corpus.
    \item The training loss curve (Figure~3.8) decreases smoothly without large oscillations, and the validation F1, precision, and recall curves (Figure~3.9) saturate and stabilize after the early epochs, without evidence of severe overfitting on the validation folds.
\end{itemize}

This stability is largely attributable to the training strategies in Section~2.3.2 and the ablation results in Table~3.5:
\begin{enumerate}
    \item \textbf{Differential and Layer-wise Learning Rates} \\
    Small learning rates for lower transformer layers (down to $2\times10^{-5}$ with layer-wise decay) protect general linguistic knowledge acquired during pre-training, while larger learning rates for the CRF and classification head (up to $2\times10^{-3}$) allow rapid adaptation of task-specific components. Ablation results indicate that removing layer-wise decay modestly degrades both mean F1 and its stability across folds.

    \item \textbf{R-Drop Regularization} \\
    By enforcing consistency between two stochastic forward passes via a KL-divergence penalty, R-Drop smooths the decision boundary and reduces over-confidence on noisy examples. This regularization helps explain the absence of sharp performance drops across evaluation checkpoints.

    \item \textbf{Exponential Moving Average (EMA) of Weights} \\
    Using EMA-smoothed parameters for evaluation further dampens the effect of noisy gradient updates, yielding more stable validation trajectories and slightly better generalization compared to using raw checkpoint weights alone, as reflected in the ablation study.
\end{enumerate}

Together, these techniques enable a compact encoder + CRF stack to train reliably, achieving competitive performance with a significantly smaller computational footprint than RoBERTa.

\subsection{Dimensionality Reduction and Clustering}
To visually inspect the learned representations, we extracted the hidden states of entity tokens from the validation set and projected them into 2D space using both PCA and UMAP. We compare the embeddings learned by our DistilBERT-CRF model against the BiLSTM-CRF baseline to demonstrate the superior representational capacity of the transformer encoder.

\begin{figure}[h]
    \centering
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/PCA.png}
        \caption{BiLSTM-CRF Embeddings (PCA)}
        \label{fig:bilstm_pca}
    \end{minipage}
    \hfill
    \begin{minipage}{0.48\textwidth}
        \centering
        \includegraphics[width=\linewidth]{figures/DistiBERT-CRF/pca_embeddings.png}
        \caption{DistilBERT-CRF Embeddings (PCA)}
        \label{fig:distilbert_pca}
    \end{minipage}
\end{figure}

\begin{figure}[h]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/DistiBERT-CRF/umap_embeddings.png}
    \caption{DistilBERT-CRF Embeddings (UMAP). This non-linear projection reveals tight, well-separated clusters for distinct entity types, with representative tokens annotated.}
    \label{fig:distilbert_umap}
\end{figure}

The visualizations demonstrate a clear qualitative difference between the two architectures:

\begin{itemize}
    \item \textbf{Cluster Separability (BiLSTM vs. DistilBERT)}:
    Figure~\ref{fig:bilstm_pca} (BiLSTM-CRF) shows a relatively diffuse distribution where entity clusters (e.g., ORG vs. LOC) overlap significantly in the center. In contrast, Figure~\ref{fig:distilbert_pca} (DistilBERT-CRF) exhibits a much more structured embedding space with clearer boundaries between major entity types. This confirms that the pre-trained transformer encoder provides far richer and more discriminative contextual features than the static embeddings used in BiLSTM.

    \item \textbf{Fine-grained Structure (UMAP Analysis)}:
    The UMAP projection in Figure~\ref{fig:distilbert_umap} further highlights the quality of DistilBERT's representations. We observe:
    \begin{itemize}
        \item \textbf{Sharp Boundaries}: Clusters for Person (PER) and Location (LOC) are extremely compact and well-separated.
        \item \textbf{Semantic Overlap}: The embeddings for Organization (ORG) and Miscellaneous (MISC) show closer proximity and some overlap. This aligns perfectly with our error analysis, which identified ORG $\leftrightarrow$ MISC confusion as a primary error source.
        \item \textbf{Contextual Grouping}: The annotated tokens reveal that ambiguous terms are often grouped by their context rather than just surface form, demonstrating the model's ability to perform word sense disambiguation in the embedding space.
    \end{itemize}

    \item \textbf{Quantitative Metric}:
    To validate these visual observations, we computed the \textbf{Homogeneity Score} of K-Means clustering ($k=7$) on the DistilBERT embeddings. The score of \textbf{0.8523} quantitatively confirms that the learned representations are highly consistent with ground-truth entity labels, supporting the high classification performance observed in Chapter 3.
\end{itemize}

Together, these results provide strong evidence that the performance gap between BiLSTM and Transformer models is rooted in the fundamental quality of their feature representations.

\section{Limitations and Threats to Validity}

While the DistilBERT-CRF model establishes a strong baseline, several limitations and threats to validity should be noted:

\begin{itemize}
    \item \textbf{CV--Test Gap}: There is a noticeable gap between the cross-validation mean F1 ($\approx 92.8\%$) and the final test F1 ($\approx 89.6\%$). This suggests some degree of overfitting to the validation folds or a distribution mismatch between the development and test splits of CoNLL-2003. Future work could investigate more conservative early stopping, stronger regularization, or alternative data partitioning strategies.

    \item \textbf{Impact of Augmentation}: The ablation study in Chapter~3 indicates that entity-aware augmentation yielded negligible aggregate gains in cross-validation. This implies that the current replacement strategy might be injecting too much noise, or that the model is already robust to the variations introduced. Designing more targeted augmentations or curriculum-style augmentation schedules could be a promising direction.

    \item \textbf{Metric Coverage and Structural Errors}: The current evaluation primarily focuses on strict entity-level F1. We did not explicitly log illegal sequence rates (e.g., \texttt{I-ORG} following \texttt{B-PER}) in the main results table. Although the CRF layer theoretically suppresses such transitions, quantifying the reduction in structural errors compared to a non-CRF baseline would strengthen the architectural justification.

    \item \textbf{Implementation-level Sensitivity}: We did not perform an extensive sensitivity analysis with respect to random seeds, different optimizers, or alternative learning-rate schedulers. The reported variance across folds partly reflects this, but additional experiments would be required to fully characterize the robustness of the training pipeline.

    \item \textbf{Domain Generalization}: The results are specific to the CoNLL-2003 dataset (news domain). The transferability of the specific hyperparameters (e.g., aggressive learning-rate differentials) and architectural choices (e.g., BIOES with CRF) to other domains such as biomedical or social media text remains unverified.
\end{itemize}

\section{Summary and Future Directions}

Across all experiments, several conclusions emerge:

\begin{itemize}
    \item Pretrained transformers are crucial for state-of-the-art NER performance. Both RoBERTa and DistilBERT-CRF substantially outperform the non-pretrained BiLSTM-CRF baseline, confirming the value of large-scale pretraining for contextual representation learning.

    \item Structured CRF decoding is a key component when model capacity is limited or label imbalance is severe. For BiLSTM, removing CRF collapses entity-level F1 to about $6$--$7\%$; with a CRF, the same encoder becomes a competitive baseline. Even for DistilBERT, the CRF layer improves BIO/BIOES consistency and helps enforce valid transitions.

    \item Most remaining errors are type confusion rather than boundary errors. RoBERTa and DistilBERT-CRF largely solve span detection but still misclassify ambiguous phrases and over-predict entity-like surface forms as entities.
\end{itemize}

Future work should therefore focus on mitigating over-generalization and semantic type confusion, for example by:
\begin{itemize}
    \item integrating external lexical or knowledge resources,
    \item designing contrastive objectives that separate entity types in representation space, and
    \item performing targeted data augmentation or hard-negative mining for non-entity spans that closely resemble entities.
\end{itemize}

Such extensions would complement the current architectures and training strategies, and may further close the gap between strong baselines and fully robust, domain-transferable NER systems.