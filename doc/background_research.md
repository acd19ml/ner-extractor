## 命名实体识别（NER）的深度研究

### 背景和重要性

命名实体识别（Named Entity Recognition, NER）是自然语言处理中的一项核心任务，用于从文本中自动提取具有特定意义的命名实体（如人名、组织、地点、日期等）。简单来说，NER 将非结构化的原始文本转化为结构化信息：例如在一篇新闻中提到“Elon Musk”、“Tesla”和“2023”，NER 会识别出“Elon Musk”是人名（PER），“Tesla”是组织名（ORG），“2023”是日期。通过这样的实体识别，文本中的关键信息被结构化地提取出来。

NER 之所以重要，在于它能将凌乱的自然语言转变为有用的数据。几乎所有涉及文本理解的应用背后都有 NER 的身影——从搜索引擎根据你的查询识别人物或地点，到客服聊天机器人捕捉用户提到的产品或问题领域，都依赖实体识别来提取关键内容。NER 广泛应用于医疗（如从临床报告中提取疾病和治疗措施）、金融（从新闻中捕获公司名称、数字信息）、电商（识别产品名、价格）以及法律（标注合同中的法规、日期、机构）等领域。可以说，只要有文本分析的场景，NER 提供的结构化实体信息就极大提升了系统理解语言的能力。

### NER 任务与序列标注

NER 通常被建模为序列标注任务，即对输入文本的每个标记（token）赋予一个标签，指示其是否属于某类命名实体。这通常采用 BIO（或其扩展如 BIOES）标注方案，其中 `B-` 表示实体开头，`I-` 表示实体内部，`O` 表示不属于任何命名实体。例如，英语句子 “Mark Watney visited Mars”（马克·沃特尼访问了火星）的标注结果如下：

- Mark `B-PER`（人名开头）
- Watney `I-PER`（人名内部）
- visited `O`（非实体）
- Mars `B-LOC`（地点名开头）

通过这种逐词标注，模型可以识别出连续的标记序列组成完整的实体片段。

在标准的数据集中，文本中的实体类别和标注格式都有明确规定。著名的 CoNLL-2003 英语数据集（源自路透社新闻）就定义了四种命名实体类别：人名（PER）、组织（ORG）、地点（LOC）和其他杂项（MISC）。该数据使用 IOB 格式对每个单词进行标注（即在实体开头标 `B-` 标签，内部标 `I-`，其余标 `O`），并假定命名实体不重叠嵌套。有了统一的标注方案和数据，研究者可以训练和评估不同的 NER 模型，在如 CoNLL-2003 这样的基准上比较性能。

### 传统方法：规则驱动和统计模型

在深度学习兴起之前，NER 系统主要依赖于基于规则和特征工程的方法。早期的方法会利用人工制定的规则和词典（又称特征模板或挂接表）来识别人名、地名等，例如“如果一个单词以大写字母开头，可能是人名”。再比如，如果某词出现在称呼（如 “Mr.”、“Ms.”）之后，也可能是人名。这类规则驱动的方法在特定领域有效，但往往刚性且不具备通用性：语言中的复杂情况很多，单靠固定规则无法应对所有场景。同时，基于词典的方式对未收录的新实体（如新出现的人名或公司）无能为力，也难以判断歧义词的正确类别。

为了解决规则方法的局限，研究者引入了监督学习的统计模型来自动完成标注任务。其中具有代表性的是隐马尔可夫模型（HMM）和条件随机场（CRF）等序列模型。这些模型通过在标注语料上训练，学习在给定上下文时预测标签序列的概率。尤其是 CRF，作为判别式序列模型，能够结合手工设计的特征（如单词是否大写、词性、上下文窗口内的词等）来提高预测准确性。早期很多 NER 系统就是基于 CRF 配合丰富特征实现的。然而，这些传统统计模型依赖大量人工特征工程，需要领域专家设计特征模板，泛化能力受限；并且对于未登录词（训练中未见过的实体）和语义歧义问题依然表现不佳。总体而言，在深度学习出现之前，NER 的性能提升在很大程度上受制于特征与规则的设计。

### 深度学习方法：BiLSTM+CRF 序列标注

2010 年代中期开始，深度学习为 NER 提供了新的解决方案，摆脱了繁琐的人工特征工程。特别是基于循环神经网络（RNN）的方法取得了突破，其中经典架构是双向 LSTM + CRF（BiLSTM-CRF）。该模型最早由 Huang 等人（2015）和 Lample 等人（2016）提出，用于序列标注任务，将双向长短期记忆网络（BiLSTM）与条件随机场（CRF）相结合。

在这一架构中，BiLSTM 负责从两个方向（前向和后向）编码句子的上下文信息。相比单向的 RNN，双向 LSTM 能够“记住”目标实体左侧和右侧的单词，有效捕捉长程依赖，从而更准确地识别实体边界和类型。而在输出层引入 CRF 则可以利用序列层面的信息对标注结果做整体优化：CRF 会查看整个标签序列，确保其合法且一致，例如不会出现 “New” 被标为地点而紧跟的 “York” 却未标注的不一致情况。LSTM 捕获上下文、CRF 全局约束，二者结合使模型既善于理解局部语境又保证输出序列的连贯合理。

这种端到端的神经序列标注模型显著提升了 NER 的效果。在无需手工特征的情况下，LSTM-CRF 模型通过字符和词嵌入自动学习特征，达到了当时最先进的性能：例如 Lample 等人（2016）的双向 LSTM-CRF 在 CoNLL-2003 英语测试集上取得了约 90.9% 的 F1 分数，相比早期基于 CRF+特征的方法有大幅度提升。ELMo 等预训练词表示方法引入后，BiLSTM-CRF 的性能进一步提高到 92% 以上。

不过，RNN 类模型也存在一些固有局限。首先，由于 LSTM 必须按照序列顺序逐个词地处理文本，它难以充分并行化，训练和推理速度受到限制。同时，尽管 LSTM 引入了门机制缓解长期依赖难题，但对于特别长的句子或跨句依存，它可能仍然存在信息损失或梯度衰减的问题。因此，业界开始探索新的网络架构，以进一步提升 NER 的效果和效率。

### Transformer 与预训练模型：BERT 等的应用

近年来，Transformer 架构的出现为 NER 提供了更强大的建模能力。Transformer（Vaswani 等人，2017）采用多头自注意力机制，可以全局建模序列中词与词之间的关系，克服了 RNN 顺序处理的限制。以 BERT 为代表的预训练 Transformer 模型在 NER 任务中取得了突破性的成果。

与 LSTM 必须按字序逐步阅读句子不同，BERT 可以在一次前向传递中“同时看见”整个句子的所有单词，并通过自注意力计算每个词对其他词的相关性。这意味着模型能够充分利用实体前后的双向上下文来判别其类型——就如同一次扫视整句而非逐词阅读。例如，在句子 “Apple is launching a new product.” 中，BERT 利用了 “launching” 和 “product” 等上下文词来判断 “Apple” 指的是公司而不是水果，这种上下文嵌入使模型能够掌握词语细微的语义区别，因而效果显著优于只能局部看的旧方法。

更重要的是，BERT 等模型通常经过大规模语料的预训练，然后通过微调应用于下游 NER 任务。预训练让模型掌握了一般语言知识（例如词语含义、多义词消歧等），微调则在少量标注数据上调整模型参数，使之适应特定的 NER 标签空间。这种迁移学习的范式极大降低了对大规模标注数据的依赖，却能取得极高的准确率。研究显示，只需几轮微调，预训练的 BERT 在 CoNLL-2003 上就达到了约 92.8% 的 F1 分值（Devlin 等人，2019），一举刷新了该基准的性能纪录。相比之下，同一数据集上传统 BiLSTM-CRF 的表现约为 90–91% F1，足见 Transformer 架构结合预训练的威力。

BERT 的成功催生了许多变体和改进模型在 NER 上的应用：例如更大规模预训练的 RoBERTa、轻量化的 DistilBERT，以及结合字符级表示的 Flair 等都进一步提高了 NER 的效果。某些最新模型甚至利用额外的上下文或知识，将 CoNLL-2003 的实体识别 F1 提升到 94–95%。当然，这些 Transformer 模型往往拥有数亿参数，在性能跃升的同时也带来了模型体积大、计算开销高的挑战（尤其在长文本上自注意力的计算复杂度随长度呈二次增长）。

### 基于提示的大型语言模型方法

除了直接对每个词做分类的范式，近年兴起了利用大型预训练生成模型（Large Language Models, LLMs）通过提示（prompt）来完成 NER 的新思路。这类方法不再训练一个专门的分类器，而是借助像 GPT-3 或 GPT-4 这样通用的大型语言模型，通过设计自然语言提示引导模型输出包含实体的结果。例如，我们可以提供一个提示：「请找出以下文本中的所有人名、组织名和地点：…」，让预训练模型直接生成识别出的实体及其类别。

在学术上，Prompt 学习被视为一种新范式：与传统监督学习需要明确的标签输出不同，提示方法依赖语言模型根据概率生成文本来推断答案。通常，我们先定义一个提示模板（例如句子中挖空要识别的实体），将原始输入插入提示中，由语言模型填空生成包含实体的回答。根据实现方式的不同，又可分为基于模板的离散提示、无需模板的自由提示、连续软提示以及问答式的方法等多种变体。例如有工作将 NER 转化为一系列问题：“文本中提到的人名是什么？”、“提到的组织名是什么？”，由模型逐一作答，这实质上是将 NER 转换为了阅读理解/问答问题来利用 LLM 的语言理解能力。

提示式 NER 方法的潜在优势在于大模型已经在海量文本上学到了丰富的知识，因此可以实现零样本或小样本（few-shot）学习。对于一些缺乏标注数据的领域（如新兴术语、低资源语言），我们无需训练一个新模型，只要精心编写提示，大模型就有可能给出不错的结果。这为快速构建 NER 系统提供了一条新途径。

然而，目前基于提示的方式在准确性和可靠性上还难以媲美专门微调的模型。一些研究表明，即使是最强大的通用 LLM，在细粒度的实体识别任务上，其性能往往不如在同类数据上微调过的专用模型。这可能是因为大模型生成答案时，难以保证每个相关实体都被完整捕获且标签准确。此外，提示设计本身是门学问：模型对提示的表述非常敏感，需要尝试不同措辞、顺序才能得到最佳结果。提示工程成本高、对模型预测缺乏稳定控制，以及在迁移到新任务时缺少鲁棒性，都是当前 Prompt-NER 方法面临的挑战。再者，从技术实现角度看，调用大型模型进行推理的计算代价极高，模型可能产生不一致或虚假的输出，还存在潜在的偏见问题。因此，尽管 Prompt 方法很有前景，但在实际应用中需要权衡其优势和局限。

### 不同方法的对比分析

各类 NER 实现方案各有特点和适用情景，下面对主要方法做一个总结比较：

- 基于规则和特征工程的方法：不依赖大规模数据，利用人为知识直接构建识别系统，具有一定的可解释性和领域适应性。但缺点是覆盖面有限，开发维护成本高，难以处理未见过的新实体和复杂的语言现象。性能上，这类方法的上限相对较低，一般需配合机器学习模型才能达到可用效果。
- BiLSTM+CRF 神经序列标注：端到端学习文本特征，无需人工制定规则，能够自动捕捉上下文模式，显著提高了识别准确率。尤其是引入预训练词嵌入后，LSTM-CRF 模型达到了以前不可想象的性能水平（接近九成 F1）。其不足在于顺序计算难以并行，训练速度较慢，对长距离依赖的捕获仍有一定局限。在资源受限场景下，训练大型 LSTM 模型也可能遇到困难。
- 基于 Transformer 的预训练模型：如 BERT 等模型利用海量语料预训练，拥有强大的上下文建模能力，对实体的多义性理解更深刻。微调后的 BERT 在标准基准上精度最高，可达到 92–93% 以上的 F1。另外，Transformer 支持并行计算，对长文本也有较好的建模效果。然而，这类模型参数规模庞大，计算和存储成本高，在实际部署时需要足够的算力支持。对于特定领域或语种，还可能需要训练或适配专门的预训练模型。
- 基于 Prompt 的大型模型方法：不需要专门训练，通过设计提示即可利用现有大模型完成任务，标注数据需求极少，适合低资源或跨语言的快速实验。这种方法也易于结合最新的通用 AI 模型能力。然而其准确性目前略逊，对提示的依赖使结果具有不确定性。另外调用大型模型本身开销巨大，响应速度和成本也是现实考虑因素。因此 Prompt 方法目前更多作为研究探索和辅助手段，在追求极致性能的场景下仍以微调专用模型为主。

### 总结

NER 技术已经经历了从基于规则的可控但僵化，到统计学习的自动化，再到深度学习时代的上下文感知和预训练迁移的演变。在实际应用中，需要根据任务需求权衡选择：如果追求最高精度且有充足算力，优先考虑基于 BERT 等 Transformer 的模型；如果数据有限希望快速构建原型，可以尝试利用大型模型通过提示完成 NER；而在计算资源受限或需要解释性的场景，下迁的轻量级模型（如 BiLSTM-CRF）仍不失为一种平衡选择。随着研究的推进，NER 的方法还在不断丰富，例如结合生成式和判别式模型、处理嵌套实体和跨领域迁移等方向都取得进展。未来，我们有望看到更加通用且高效的 NER 系统，能够适应任何语言和领域的实体识别需求。